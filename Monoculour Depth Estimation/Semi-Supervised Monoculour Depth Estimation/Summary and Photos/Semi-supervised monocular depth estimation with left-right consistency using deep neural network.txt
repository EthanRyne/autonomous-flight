Abstract

There has been tremendous research progress in
estimating the depth of a scene from a monocular camera
image. Existing methods for single-image depth prediction are
exclusively based on deep neural networks, and their training
can be unsupervised using stereo image pairs, supervised using
LiDAR point clouds, or semi-supervised using both stereo and
LiDAR. In general, semi-supervised training is preferred as
it does not suffer from the weaknesses of either supervised
training, resulting from the difference in the cameras and
the LiDARs field of view, or unsupervised training, resulting
from the poor depth accuracy that can be recovered from a
stereo pair. In this paper, we present our research in singleimage depth prediction using semi-supervised training that
outperforms the state-of-the-art. We achieve this through a
loss function that explicitly exploits left-right consistency in a
stereo reconstruction, which has not been adopted in previous
semi-supervised training. In addition, we describe the correct
use of ground truth depth derived from LiDAR that can
significantly reduce prediction error. The performance of our
depth prediction model is evaluated on popular datasets, and
the importance of each aspect of our semi-supervised training
approach is demonstrated through experimental results. Our
deep neural network model has been made publicly available.